## 2025.10.9 
初步实现了gpt2-137M的朴素实现，但回答问题时答非所问

## 2025.10.10
将top-k调整到40-50效果达到正常<br>
模型att的输出与官方attn输出的相对容忍度在1e-05的条件下始终为false，但是可以通过1e-04

## 2025 10.10
初步添加了批处理功能，功能不成熟，输出时暂时不能判断eos

## 2025.10.11
完善了批处理功能

## 2025.10.17
在原模型的基础上加入了KV-cache功能。<br>
对于同样的prompt,decode 900次 由25s -> 11s(之后的测试都用这个prompt,seed,后面不再赘述) <br>
另外GPT2-137M 存放KV-cache的空间如下计算：<br>
token_kvcache_size = 2 * n_layer * head_dim * 4(f32) = 2 * 12 * 768 * 4 = 73728B<br>
seq_kvcache_size = max_seq_size * token_kvcache_size = 1024 * 73728B = 73728KB<br>
batched_kvcache_size = batch_size * seq_kvcache_size = 12 * 73728KB / 1024 = 864MB<br>

## 2025.10.28
在kv-cache的基础上增加了张量并行优化 <br>
张量并行分布在两个地方:Attention以及MLP <br>
在只对MLP进行2GPU张量并行时，由11s -> 7s <br>
但当加上Attention的张量并行时，由7s -> 8s <br>
可能原因：<br>
1.1次reduce变为2次reduce，通信开销增加  <br>
2.gpt2的qkv一起加载到同一矩阵，我在处理时切分成了三个矩阵(未加张量并行时没有切分，这里考虑到好实现因拆分)，因此从启动一个内核变为启动三个内核增加开销，且三次矩阵乘法的时间开销也远高于一次矩阵乘法 <br>
3.代码结构不佳，拖慢效率    <br>
<br>
后续计划：
1.优化代码结构      <br>
2.qkv融合       <br>
3.词表并行      <br>

## 2025.10.30
qkv融合，由原来的三次矩阵计算变为一次矩阵计算，平均能加快0.2s-0.3s  <br>

## 2025.10.31
原来的KVcache是动态分配的，即每算出一个token，就用torch.cat拼接到KVcache的末尾  <br>
但这并不是一个好的方法，原因如下：  <br>
当内存不充裕时，若存放KVcache的碎片已经存满，但是推理还未结束，那么系统将会找出一个更大的碎片，然后把填满的碎片的内存移动过去，这无疑很浪费时间 <br>
此外，若没有更大的碎片，系统可能会对内存进行swap，造成更大的浪费

